{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1amduCKA67qgXRDKdr_c_z3NHVTRsWFT4",
      "authorship_tag": "ABX9TyOZcFeaXW5628FH+dPR+pQB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leon-web-net/Computer_Vision/blob/main/YOLO_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Change Directory/albumentations/Config_list"
      ],
      "metadata": {
        "id": "ufbyeKD7LZjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Change to a new directory\n",
        "new_directory = '/content/drive/MyDrive/YOLOV3_Pytorch/'\n",
        "os.chdir(new_directory)\n",
        "\n",
        "# Confirm the changey\n",
        "print(\"New working directory:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqzh8Ii3LXaj",
        "outputId": "10e8b884-31af-43f4-c416-08d62e405032"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New working directory: /content/drive/MyDrive/YOLOV3_Pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall albumentations\n",
        "!pip install albumentations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuD5iutwGIMV",
        "outputId": "8be2eb1e-19b6-4b5d-e69a-04a560432383"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: albumentations 1.4.20\n",
            "Uninstalling albumentations-1.4.20:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/albumentations-1.4.20.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/albumentations/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled albumentations-1.4.20\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-1.4.24-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.10.3)\n",
            "Collecting albucore==0.0.23 (from albumentations)\n",
            "  Downloading albucore-0.0.23-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.23->albumentations) (3.11.1)\n",
            "Collecting simsimd>=5.9.2 (from albucore==0.0.23->albumentations)\n",
            "  Downloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (4.12.2)\n",
            "Downloading albumentations-1.4.24-py3-none-any.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading albucore-0.0.23-py3-none-any.whl (14 kB)\n",
            "Downloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (632 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m632.7/632.7 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: simsimd, albucore, albumentations\n",
            "  Attempting uninstall: albucore\n",
            "    Found existing installation: albucore 0.0.19\n",
            "    Uninstalling albucore-0.0.19:\n",
            "      Successfully uninstalled albucore-0.0.19\n",
            "Successfully installed albucore-0.0.23 albumentations-1.4.24 simsimd-6.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Albumentation Edit"
      ],
      "metadata": {
        "id": "0tdxzPZoLPhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "\n",
        "# Replace the file from Google Drive\n",
        "src = \"/content/drive/MyDrive/YOLOV3_Pytorch/bbox_utils.py\"\n",
        "dst = \"/usr/local/lib/python3.10/dist-packages/albumentations/core/bbox_utils.py\"\n",
        "\n",
        "shutil.copy(src, dst)\n",
        "print(\"File replaced successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9HfD8ePKtFX",
        "outputId": "2874ef24-ea47-4561-d5fd-98ae090a6455"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File replaced successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Information about architecture config:\n",
        "Tuple is structured by (filters, kernel_size, stride)\n",
        "Every conv is a same convolution.\n",
        "List is structured by \"B\" indicating a residual block followed by the number of repeats\n",
        "\"S\" is for scale prediction block and computing the yolo loss\n",
        "\"U\" is for upsampling the feature map and concatenating with a previous layer\n",
        "\"\"\"\n",
        "\n",
        "config_list = [\n",
        "    (32, 3, 1),\n",
        "    (64, 3, 2),\n",
        "    [\"B\", 1],\n",
        "    (128, 3, 2),\n",
        "    [\"B\", 2],\n",
        "    (256, 3, 2),\n",
        "    [\"B\", 8],\n",
        "    (512, 3, 2),\n",
        "    [\"B\", 8],\n",
        "    (1024, 3, 2),\n",
        "    [\"B\", 4],  # To this point is Darknet-53\n",
        "    (512, 1, 1),\n",
        "    (1024, 3, 1),\n",
        "    \"S\",\n",
        "    (256, 1, 1),\n",
        "    \"U\",\n",
        "    (256, 1, 1),\n",
        "    (512, 3, 1),\n",
        "    \"S\",\n",
        "    (128, 1, 1),\n",
        "    \"U\",\n",
        "    (128, 1, 1),\n",
        "    (256, 3, 1),\n",
        "    \"S\",\n",
        "]"
      ],
      "metadata": {
        "id": "ARKfAcC858pt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "lo4-Y10PI89U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsMBs8StNAzq",
        "outputId": "9043b4c4-3791-44d0-f3fd-642adf2bf393"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class CNNBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs)\n",
        "    self.bn = nn.BatchNorm2d(out_channels)\n",
        "    self.leaky = nn.LeakyReLU(0.1)\n",
        "    self.use_bn_act = bn_act\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.use_bn_act:\n",
        "      return self.leaky(self.bn(self.conv(x)))\n",
        "    else:\n",
        "      return self.conv(x)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self, channels, use_residual=True, num_repeats=1):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList()\n",
        "    for repeat in range(num_repeats):\n",
        "      self.layers +=[\n",
        "          nn.Sequential(\n",
        "          CNNBlock(channels, channels//2, kernel_size=1),\n",
        "          CNNBlock(channels//2, channels, kernel_size=3, padding=1),\n",
        "          )\n",
        "      ]\n",
        "      self.use_residual = use_residual\n",
        "      self.num_repeats = num_repeats\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x) + x if self.use_residual else layer(x)\n",
        "    return x\n",
        "\n",
        "class ScalePrediction(nn.Module):\n",
        "  def __init__(self, in_channels, num_classes):\n",
        "    super().__init__()\n",
        "    self.pred = nn.Sequential(\n",
        "        CNNBlock(in_channels, 2*in_channels, kernel_size=3, padding=1),\n",
        "        CNNBlock(2*in_channels, 3*(5+num_classes), bn_act=False, kernel_size=1),  # [po, x, y, w, h]\n",
        "    )\n",
        "\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "  def forward(self,x):\n",
        "    return(\n",
        "        self.pred(x).reshape(\n",
        "            x.shape[0],3, self.num_classes+5, x.shape[2], x.shape[3]\n",
        "        ).permute(0,1,3,4,2)  # change order\n",
        "    )\n",
        "\n",
        "class YOLOv3(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels=3, num_classes=10):\n",
        "    super().__init__()\n",
        "    self.num_classes = num_classes\n",
        "    self.in_channels = in_channels\n",
        "    self.layers = self._create_conv_layers()\n",
        "\n",
        "  def forward(self,x):\n",
        "    outputs = []\n",
        "    route_connections = []\n",
        "\n",
        "    for layer in self.layers:\n",
        "      if isinstance(layer, ScalePrediction):\n",
        "        outputs.append(layer(x))\n",
        "        continue\n",
        "      x = layer(x)\n",
        "\n",
        "      if isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n",
        "        route_connections.append(x)\n",
        "\n",
        "      elif isinstance(layer, nn.Upsample):\n",
        "        x = torch.cat([x, route_connections[-1]], dim=1)\n",
        "        route_connections.pop()\n",
        "\n",
        "    return outputs\n",
        "\n",
        "  def _create_conv_layers(self):\n",
        "    layers = nn.ModuleList()\n",
        "    in_channels = self.in_channels\n",
        "\n",
        "    for module in config_list:\n",
        "      if isinstance(module, tuple):\n",
        "        out_channels, kernel_size, stride = module\n",
        "        layers.append(CNNBlock(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size = kernel_size,\n",
        "            stride = stride,\n",
        "            padding = 1 if kernel_size == 3 else 0,\n",
        "        ))\n",
        "        in_channels = out_channels\n",
        "\n",
        "      elif isinstance(module, list):\n",
        "        num_repeats = module[1]\n",
        "        layers.append(ResidualBlock(in_channels, num_repeats=num_repeats,))\n",
        "\n",
        "      elif isinstance(module, str):\n",
        "        if module == \"S\":\n",
        "          layers+=[\n",
        "              ResidualBlock(in_channels, use_residual=False, num_repeats=1),\n",
        "              CNNBlock(in_channels, in_channels//2, kernel_size=1),\n",
        "              ScalePrediction(in_channels//2, num_classes=self.num_classes),\n",
        "          ]\n",
        "\n",
        "          in_channels = in_channels // 2\n",
        "\n",
        "        elif module == \"U\":\n",
        "          layers.append(nn.Upsample(scale_factor=2),)\n",
        "          in_channels = in_channels * 3\n",
        "\n",
        "    return layers\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  num_classes = 10\n",
        "  IMAGE_SIZE = 416\n",
        "  model = YOLOv3(num_classes=num_classes)\n",
        "  x = torch.randn((2,3,IMAGE_SIZE,IMAGE_SIZE))\n",
        "  out = model(x)\n",
        "  assert model(x)[0].shape == (2,3,IMAGE_SIZE//32, IMAGE_SIZE//32, num_classes+5)\n",
        "  assert model(x)[1].shape == (2,3,IMAGE_SIZE//16, IMAGE_SIZE//16, num_classes+5)\n",
        "  assert model(x)[2].shape == (2,3,IMAGE_SIZE//8, IMAGE_SIZE//8, num_classes+5)\n",
        "  print(\"Success!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "ZMRf1t2mNiVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from PIL import Image, ImageFile\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from utils import iou_width_height, intersection_over_union, non_max_suppression\n"
      ],
      "metadata": {
        "id": "I7hfRviO3WGn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Loader"
      ],
      "metadata": {
        "id": "x8WmcPvENofE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "# class YOLODataset(Dataset):\n",
        "#   def __init__(self,\n",
        "#                csv_file,\n",
        "#                img_dir,label_dir,\n",
        "#                anchors,\n",
        "#                S= [13,26,52],\n",
        "#                C=20,\n",
        "#                transform=None):\n",
        "#     self.annotations = pd.read_csv(csv_file)\n",
        "#     self.img_dir = img_dir\n",
        "#     self.label_dir = label_dir\n",
        "#     self.transform = transform\n",
        "#     self.S = S\n",
        "#     self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2])\n",
        "#     self.num_anchors = self.anchors.shape[0]\n",
        "#     self.num_anchors_per_scale = self.num_anchors // 3\n",
        "#     self.C = C\n",
        "#     self.ignore_iou_thresh = 0.5\n",
        "\n",
        "#   def __len__(self):\n",
        "#     return len(self.annotations)\n",
        "\n",
        "#   def __getitem__(self, index):\n",
        "#     label_path = os.path.join(self.label_dir, self.annotations.iloc[index,1])\n",
        "#     # [x y w h class]\n",
        "#     bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n",
        "#     img_path = os.path.join(self.img_dir, self.annotations.iloc[index,0])\n",
        "#     image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "\n",
        "#     # Convert bounding boxes from [x_center, y_center, width, height] -> [x_min, y_min, x_max, y_max]\n",
        "#     # bboxes = [\n",
        "#     #     [\n",
        "#     #         b[0] - b[2] / 2,  # x_min\n",
        "#     #         b[1] - b[3] / 2,  # y_min\n",
        "#     #         b[0] + b[2] / 2,  # x_max\n",
        "#     #         b[1] + b[3] / 2,  # y_max\n",
        "#     #         b[4],             # class\n",
        "#     #     ]\n",
        "#     #     for b in bboxes\n",
        "#     # ]\n",
        "\n",
        "#     if self.transform:\n",
        "#       print(f\"self transform: {self.transform}, this should be none\")\n",
        "#       augmentations = self.transform(image= image, bboxes=bboxes)\n",
        "#       image = augmentations[\"image\"]\n",
        "#       bboxes = augmentations[\"bboxes\"]\n",
        "\n",
        "#     # P_O (probability of object) =>  [p_o, x, y, w, h, c]\n",
        "#     targets = [torch.zeros((self.num_anchors // 3, S, S, 6)) for S in self.S]\n",
        "#     boxes = []\n",
        "\n",
        "#     for box in bboxes:\n",
        "#       iou_anchors = iou_width_height(torch.tensor(box[2:4]), self.anchors)\n",
        "#       anchor_indices = iou_anchors.argsort(descending=True, dim=0)\n",
        "#       x, y, width, height, class_label = box\n",
        "#       has_anchor = [False,False, False]\n",
        "\n",
        "#       for anchor_idx in anchor_indices:\n",
        "#         # 0,1,2 which scale\n",
        "#         scale_idx = anchor_idx // self.num_anchors_per_scale\n",
        "#         # 0,1,2 which anchor on scale\n",
        "#         anchor_on_scale = anchor_idx % self.num_anchors_per_scale\n",
        "#         S = self.S[scale_idx] # how many cell in scale\n",
        "#         i,j = int(S*y), int(S*x) # x =0.5, S=13\n",
        "#         anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n",
        "#         if not anchor_taken and not has_anchor[scale_idx]:\n",
        "#           targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n",
        "#           x_cell, y_cell = S * x - j, S * y - i\n",
        "#           width_cell, height_cell = (\n",
        "#               width * S,\n",
        "#               height * S,\n",
        "#           )\n",
        "\n",
        "#           box_coordinates = torch.tensor(\n",
        "#               [x_cell, y_cell, width_cell, height_cell]\n",
        "#           )\n",
        "#           targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n",
        "#           targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n",
        "#           has_anchor[scale_idx] = True\n",
        "\n",
        "#         elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n",
        "#           targets[scale_idx][anchor_on_scale, i, j, 0] = -1 # ignore prediction\n",
        "\n",
        "#     return image, tuple(targets)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "code",
        "id": "E54v_0wmNneV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Function"
      ],
      "metadata": {
        "id": "q5mlEtsRJieA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from utils import intersection_over_union\n",
        "\n",
        "class YoloLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.mse = nn.MSELoss()\n",
        "    self.bce = nn.BCEWithLogitsLoss()\n",
        "    self.entropy = nn.CrossEntropyLoss()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # Constants\n",
        "    self.lambda_class = 1\n",
        "    self.lambda_noobj = 10\n",
        "    self.lambda_obj = 1\n",
        "    self.lambda_box = 10\n",
        "\n",
        "\n",
        "  def forward(self, predictions, target, anchors):\n",
        "    obj = target[...,0] == 1\n",
        "    noobj = target[...,0] == 0\n",
        "\n",
        "    # No object loss\n",
        "    no_object_loss = self.bce(\n",
        "        (predictions[...,0:1][noobj]), (target[...,0:1][noobj]),\n",
        "    )\n",
        "\n",
        "    # Object Loss\n",
        "    anchors = anchors.reshape(1,3,1,1,2)\n",
        "    box_preds = torch.cat([self.sigmoid(predictions[...,1:3]), torch.exp(predictions[...,3:5])*anchors], dim=-1)\n",
        "    ious = intersection_over_union(box_preds[obj], target[...,1:5][obj]).detach()\n",
        "    object_loss = self.bce((predictions[...,0:1][obj]),(ious*target[...,0:1][obj]))\n",
        "\n",
        "    # Box Coordinate Loss\n",
        "    predictions[...,1:3] = self.sigmoid(predictions[...,1:3]) # x,y to be [0,1]\n",
        "    target[...,3:5] = torch.log(\n",
        "        (1e-16 + target[...,3:5] / anchors) # avoid torch log of 0 (changed from -16 -> -16)\n",
        "    )\n",
        "    box_loss = self.mse(predictions[..., 1:5][obj], target[..., 1:5][obj])\n",
        "\n",
        "    # Class Loss\n",
        "    class_loss = self.entropy(\n",
        "        (predictions[..., 5:][obj]), (target[..., 5][obj].long()),\n",
        "    ) # check class label\n",
        "\n",
        "    return (\n",
        "        self.lambda_box*box_loss\n",
        "        +self.lambda_obj*object_loss\n",
        "        +self.lambda_noobj*no_object_loss\n",
        "        +self.lambda_class*class_loss\n",
        "    )\n"
      ],
      "metadata": {
        "id": "j2BQ0peDdP12"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "uyBOfg-CA7Md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import config\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import tqdm\n",
        "\n",
        "from utils import (\n",
        "    mean_average_precision,\n",
        "    cells_to_bboxes,\n",
        "    get_evaluation_bboxes,\n",
        "    save_checkpoint,\n",
        "    load_checkpoint,\n",
        "    check_class_accuracy,\n",
        "    get_loaders,\n",
        "    plot_couple_examples,\n",
        ")\n",
        "\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "0KKOEaf5CZ-B"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Open an image file\n",
        "image = Image.open(\"/content/drive/MyDrive/datasets/YOLOV3_nuim/images/00000001.jpg\")\n",
        "\n",
        "# Get image size\n",
        "width, height = image.size\n",
        "\n",
        "print(f\"Width: {width}, Height: {height}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAqB3CUvI7t-",
        "outputId": "b3450a9a-4ded-4acf-d370-41599a437351"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Width: 1600, Height: 900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib, Test_module, utils,dataset,config\n",
        "importlib.reload(Test_module)\n",
        "importlib.reload(utils)\n",
        "from Test_module import test_fn\n",
        "importlib.reload(config)\n",
        "test_fn()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQQkPNPGnr6d",
        "outputId": "c70259aa-76cb-4af3-8be3-f8d74e210fd0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi config working? batch size\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def train_fn(train_loader,model,optimizer,loss_fn,scaler, scaled_anchors):\n",
        "  loop = tqdm.tqdm(train_loader, leave=True)\n",
        "  losses = []\n",
        "  # print(f\"\\n\\n\\nB4 FOR LOOP\\n\\n\\n\")\n",
        "  for batch_idx, (x,y) in enumerate(loop):\n",
        "\n",
        "    x = x.to(config.DEVICE)\n",
        "    y0,y1,y2 = (\n",
        "        y[0].to(config.DEVICE),\n",
        "        y[1].to(config.DEVICE),\n",
        "        y[2].to(config.DEVICE),\n",
        "    )\n",
        "\n",
        "    # print(F\"\\n\\n\\nPAST FOR LOOP\\n\\n\\n\")\n",
        "\n",
        "    with torch.cuda.amp.autocast(): # float 16 pytorch\n",
        "      out = model(x)\n",
        "      loss = (\n",
        "          loss_fn(out[0], y0, scaled_anchors[0])\n",
        "          + loss_fn(out[1], y1, scaled_anchors[1])\n",
        "          + loss_fn(out[2], y2, scaled_anchors[2])\n",
        "      )\n",
        "\n",
        "    losses.append(loss.item())\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    # update progress bar\n",
        "    mean_loss = sum(losses)/len(losses)\n",
        "    loop.set_postfix(loss=mean_loss)\n",
        "\n",
        "def main():\n",
        "  model = YOLOv3(num_classes=config.NUM_CLASSES).to(config.DEVICE)\n",
        "  optimizer = optim.Adam(\n",
        "      model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY\n",
        "  )\n",
        "\n",
        "  loss_fn = YoloLoss()\n",
        "  scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "  train_loader, test_loader, train_eval_loader = get_loaders(\n",
        "      train_csv_path=config.DATASET + \"/train.csv\",\n",
        "      test_csv_path=config.DATASET + \"/test.csv\",)\n",
        "\n",
        "\n",
        "  if config.LOAD_MODEL:\n",
        "    load_checkpoint(\n",
        "        config.CHECKPOINT_FILE, model, optimizer, config.LEARNING_RATE\n",
        "  )\n",
        "  scaled_anchors = (\n",
        "        torch.tensor(config.ANCHORS)\n",
        "        * torch.tensor(config.S).unsqueeze(1).unsqueeze(1).repeat(1,3,2)\n",
        "    ).to(config.DEVICE)\n",
        "\n",
        "  for epoch in range(config.NUM_EPOCHS):\n",
        "    train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors)\n",
        "\n",
        "    # if config.SAVE_MODEL:\n",
        "    #   save_checkpoint(model, optimizer, filename=f\"checkpoint.pth.tar\")\n",
        "\n",
        "    # print(f\"Currently epoch {epoch}\")\n",
        "    # print(\"On Train Eval loader:\")\n",
        "    # print(\"On Train loader:\")\n",
        "\n",
        "\n",
        "    if epoch % 5 == 0 and epoch> 0:\n",
        "      print(f\"Epoch: {epoch}, checking class accuracy\")\n",
        "      check_class_accuracy(model, test_loader, threshold=config.CONF_THRESHOLD)\n",
        "      print(\"Fiinished Checking class accuracy\")\n",
        "      pred_boxes, true_boxes = get_evaluation_bboxes(\n",
        "          test_loader,\n",
        "          model,\n",
        "          iou_threshold=config.NMS_IOU_THRESH,\n",
        "          anchors=config.ANCHORS,\n",
        "          threshold=config.CONF_THRESHOLD,\n",
        "      )\n",
        "      mapval = mean_average_precision(\n",
        "          pred_boxes,\n",
        "          true_boxes,\n",
        "          iou_threshold=config.MAP_IOU_THRESH,\n",
        "          box_format=\"midpoint\",\n",
        "          num_classes=config.NUM_CLASSES,\n",
        "      )\n",
        "      print(f\"MAP: {mapval.item()}\")\n",
        "      model.train()\n",
        "\n",
        "    if config.SAVE_MODEL:\n",
        "      save_checkpoint(model, optimizer, filename=f\"checkpoint.pth.tar\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()\n",
        "\n",
        "print(\"hi\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uRy2gdUxK2EZ",
        "outputId": "32bd7a1c-0c98-47b6-ac9e-5b057b91b9fb",
        "collapsed": true
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-729d98e7c594>:41: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]/content/drive/MyDrive/YOLOV3_Pytorch/dataset.py:78: UserWarning: loadtxt: input contained no data: \"/content/drive/MyDrive/datasets/YOLOV3_nuim/labels/00000024.txt\"\n",
            "  bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n",
            "/content/drive/MyDrive/YOLOV3_Pytorch/dataset.py:78: UserWarning: loadtxt: input contained no data: \"/content/drive/MyDrive/datasets/YOLOV3_nuim/labels/00000044.txt\"\n",
            "  bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n",
            "<ipython-input-23-729d98e7c594>:16: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # float 16 pytorch\n",
            "100%|██████████| 3/3 [00:05<00:00,  1.78s/it, loss=66.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]/content/drive/MyDrive/YOLOV3_Pytorch/dataset.py:78: UserWarning: loadtxt: input contained no data: \"/content/drive/MyDrive/datasets/YOLOV3_nuim/labels/00000044.txt\"\n",
            "  bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n",
            "/content/drive/MyDrive/YOLOV3_Pytorch/dataset.py:78: UserWarning: loadtxt: input contained no data: \"/content/drive/MyDrive/datasets/YOLOV3_nuim/labels/00000024.txt\"\n",
            "  bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=65.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]/content/drive/MyDrive/YOLOV3_Pytorch/dataset.py:78: UserWarning: loadtxt: input contained no data: \"/content/drive/MyDrive/datasets/YOLOV3_nuim/labels/00000044.txt\"\n",
            "  bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n",
            "/content/drive/MyDrive/YOLOV3_Pytorch/dataset.py:78: UserWarning: loadtxt: input contained no data: \"/content/drive/MyDrive/datasets/YOLOV3_nuim/labels/00000024.txt\"\n",
            "  bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.59it/s, loss=62.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]/content/drive/MyDrive/YOLOV3_Pytorch/dataset.py:78: UserWarning: loadtxt: input contained no data: \"/content/drive/MyDrive/datasets/YOLOV3_nuim/labels/00000044.txt\"\n",
            "  bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n",
            "/content/drive/MyDrive/YOLOV3_Pytorch/dataset.py:78: UserWarning: loadtxt: input contained no data: \"/content/drive/MyDrive/datasets/YOLOV3_nuim/labels/00000024.txt\"\n",
            "  bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.20it/s, loss=49]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]/content/drive/MyDrive/YOLOV3_Pytorch/dataset.py:78: UserWarning: loadtxt: input contained no data: \"/content/drive/MyDrive/datasets/YOLOV3_nuim/labels/00000024.txt\"\n",
            "  bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n",
            "/content/drive/MyDrive/YOLOV3_Pytorch/dataset.py:78: UserWarning: loadtxt: input contained no data: \"/content/drive/MyDrive/datasets/YOLOV3_nuim/labels/00000044.txt\"\n",
            "  bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.53it/s, loss=44.5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]/content/drive/MyDrive/YOLOV3_Pytorch/dataset.py:78: UserWarning: loadtxt: input contained no data: \"/content/drive/MyDrive/datasets/YOLOV3_nuim/labels/00000044.txt\"\n",
            "  bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n",
            "/content/drive/MyDrive/YOLOV3_Pytorch/dataset.py:78: UserWarning: loadtxt: input contained no data: \"/content/drive/MyDrive/datasets/YOLOV3_nuim/labels/00000024.txt\"\n",
            "  bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.46it/s, loss=34.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5, checking class accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class accuracy is: 12.121212%\n",
            "No obj accuracy is: 41.200058%\n",
            "Obj accuracy is: 50.649349%\n",
            "Fiinished Checking class accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch: 0 loaded\n",
            "Processing batch index 0\n",
            "Number of bounding boxes before NMS: 10647\n",
            "Box format: midpoint\n",
            "\n",
            " Before loop, No. Boxes: 6281\n",
            "\n",
            "No. Boxes: 6280\n",
            "No. Boxes: 6279\n",
            "No. Boxes: 6278\n",
            "No. Boxes: 6277\n",
            "No. Boxes: 6274\n",
            "No. Boxes: 6271\n",
            "No. Boxes: 6270\n",
            "No. Boxes: 6269\n",
            "No. Boxes: 6268\n",
            "No. Boxes: 6267\n",
            "No. Boxes: 6265\n",
            "No. Boxes: 6262\n",
            "No. Boxes: 6261\n",
            "No. Boxes: 6259\n",
            "No. Boxes: 6258\n",
            "No. Boxes: 6257\n",
            "No. Boxes: 6254\n",
            "No. Boxes: 6253\n",
            "No. Boxes: 6251\n",
            "No. Boxes: 6248\n",
            "No. Boxes: 6245\n",
            "No. Boxes: 6243\n",
            "No. Boxes: 6241\n",
            "No. Boxes: 6238\n",
            "No. Boxes: 6236\n",
            "No. Boxes: 6234\n",
            "No. Boxes: 6232\n",
            "No. Boxes: 6230\n",
            "No. Boxes: 6228\n",
            "No. Boxes: 6225\n",
            "No. Boxes: 6222\n",
            "No. Boxes: 6220\n",
            "No. Boxes: 6217\n",
            "No. Boxes: 6214\n",
            "No. Boxes: 6211\n",
            "No. Boxes: 6209\n",
            "No. Boxes: 6207\n",
            "No. Boxes: 6205\n",
            "No. Boxes: 6204\n",
            "No. Boxes: 6202\n",
            "No. Boxes: 6200\n",
            "No. Boxes: 6198\n",
            "No. Boxes: 6195\n",
            "No. Boxes: 6193\n",
            "No. Boxes: 6192\n",
            "No. Boxes: 6191\n",
            "No. Boxes: 6189\n",
            "No. Boxes: 6187\n",
            "No. Boxes: 6186\n",
            "No. Boxes: 6183\n",
            "No. Boxes: 6181\n",
            "No. Boxes: 6179\n",
            "No. Boxes: 6178\n",
            "No. Boxes: 6176\n",
            "No. Boxes: 6175\n",
            "No. Boxes: 6173\n",
            "No. Boxes: 6171\n",
            "No. Boxes: 6169\n",
            "No. Boxes: 6167\n",
            "No. Boxes: 6166\n",
            "No. Boxes: 6164\n",
            "No. Boxes: 6163\n",
            "No. Boxes: 6162\n",
            "No. Boxes: 6161\n",
            "No. Boxes: 6160\n",
            "No. Boxes: 6159\n",
            "No. Boxes: 6158\n",
            "No. Boxes: 6157\n",
            "No. Boxes: 6156\n",
            "No. Boxes: 6154\n",
            "No. Boxes: 6153\n",
            "No. Boxes: 6151\n",
            "No. Boxes: 6150\n",
            "No. Boxes: 6149\n",
            "No. Boxes: 6148\n",
            "No. Boxes: 6146\n",
            "No. Boxes: 6145\n",
            "No. Boxes: 6144\n",
            "No. Boxes: 6143\n",
            "No. Boxes: 6142\n",
            "No. Boxes: 6140\n",
            "No. Boxes: 6139\n",
            "No. Boxes: 6137\n",
            "No. Boxes: 6107\n",
            "No. Boxes: 6097\n",
            "No. Boxes: 6095\n",
            "No. Boxes: 6094\n",
            "No. Boxes: 6091\n",
            "No. Boxes: 6090\n",
            "No. Boxes: 6087\n",
            "No. Boxes: 6086\n",
            "No. Boxes: 6083\n",
            "No. Boxes: 6080\n",
            "No. Boxes: 6077\n",
            "No. Boxes: 6074\n",
            "No. Boxes: 6071\n",
            "No. Boxes: 6068\n",
            "No. Boxes: 6065\n",
            "No. Boxes: 6062\n",
            "No. Boxes: 6059\n",
            "No. Boxes: 6056\n",
            "No. Boxes: 6053\n",
            "No. Boxes: 6050\n",
            "No. Boxes: 6047\n",
            "No. Boxes: 6044\n",
            "No. Boxes: 6041\n",
            "No. Boxes: 6038\n",
            "No. Boxes: 6035\n",
            "No. Boxes: 6032\n",
            "No. Boxes: 6029\n",
            "No. Boxes: 6026\n",
            "No. Boxes: 6023\n",
            "No. Boxes: 6020\n",
            "No. Boxes: 6017\n",
            "No. Boxes: 6014\n",
            "No. Boxes: 6011\n",
            "No. Boxes: 6008\n",
            "No. Boxes: 6005\n",
            "No. Boxes: 6002\n",
            "No. Boxes: 5999\n",
            "No. Boxes: 5996\n",
            "No. Boxes: 5993\n",
            "No. Boxes: 5990\n",
            "No. Boxes: 5987\n",
            "No. Boxes: 5984\n",
            "No. Boxes: 5981\n",
            "No. Boxes: 5978\n",
            "No. Boxes: 5975\n",
            "No. Boxes: 5972\n",
            "No. Boxes: 5969\n",
            "No. Boxes: 5966\n",
            "No. Boxes: 5963\n",
            "No. Boxes: 5960\n",
            "No. Boxes: 5957\n",
            "No. Boxes: 5954\n",
            "No. Boxes: 5951\n",
            "No. Boxes: 5948\n",
            "No. Boxes: 5945\n",
            "No. Boxes: 5942\n",
            "No. Boxes: 5939\n",
            "No. Boxes: 5938\n",
            "No. Boxes: 5935\n",
            "No. Boxes: 5932\n",
            "No. Boxes: 5929\n",
            "No. Boxes: 5926\n",
            "No. Boxes: 5923\n",
            "No. Boxes: 5920\n",
            "No. Boxes: 5917\n",
            "No. Boxes: 5914\n",
            "No. Boxes: 5911\n",
            "No. Boxes: 5908\n",
            "No. Boxes: 5905\n",
            "No. Boxes: 5902\n",
            "No. Boxes: 5899\n",
            "No. Boxes: 5896\n",
            "No. Boxes: 5893\n",
            "No. Boxes: 5890\n",
            "No. Boxes: 5887\n",
            "No. Boxes: 5885\n",
            "No. Boxes: 5861\n",
            "No. Boxes: 5858\n",
            "No. Boxes: 5855\n",
            "No. Boxes: 5853\n",
            "No. Boxes: 5850\n",
            "No. Boxes: 5847\n",
            "No. Boxes: 5844\n",
            "No. Boxes: 5841\n",
            "No. Boxes: 5838\n",
            "No. Boxes: 5835\n",
            "No. Boxes: 5834\n",
            "No. Boxes: 5831\n",
            "No. Boxes: 5828\n",
            "No. Boxes: 5825\n",
            "No. Boxes: 5822\n",
            "No. Boxes: 5819\n",
            "No. Boxes: 5816\n",
            "No. Boxes: 5813\n",
            "No. Boxes: 5810\n",
            "No. Boxes: 5807\n",
            "No. Boxes: 5804\n",
            "No. Boxes: 5801\n",
            "No. Boxes: 5798\n",
            "No. Boxes: 5795\n",
            "No. Boxes: 5792\n",
            "No. Boxes: 5789\n",
            "No. Boxes: 5766\n",
            "No. Boxes: 5763\n",
            "No. Boxes: 5762\n",
            "No. Boxes: 5759\n",
            "No. Boxes: 5756\n",
            "No. Boxes: 5754\n",
            "No. Boxes: 5751\n",
            "No. Boxes: 5748\n",
            "No. Boxes: 5745\n",
            "No. Boxes: 5742\n",
            "No. Boxes: 5739\n",
            "No. Boxes: 5736\n",
            "No. Boxes: 5733\n",
            "No. Boxes: 5730\n",
            "No. Boxes: 5728\n",
            "No. Boxes: 5725\n",
            "No. Boxes: 5723\n",
            "No. Boxes: 5720\n",
            "No. Boxes: 5717\n",
            "No. Boxes: 5714\n",
            "No. Boxes: 5711\n",
            "No. Boxes: 5708\n",
            "No. Boxes: 5705\n",
            "No. Boxes: 5702\n",
            "No. Boxes: 5699\n",
            "No. Boxes: 5696\n",
            "No. Boxes: 5694\n",
            "No. Boxes: 5691\n",
            "No. Boxes: 5688\n",
            "No. Boxes: 5685\n",
            "No. Boxes: 5682\n",
            "No. Boxes: 5679\n",
            "No. Boxes: 5676\n",
            "No. Boxes: 5673\n",
            "No. Boxes: 5671\n",
            "No. Boxes: 5669\n",
            "No. Boxes: 5666\n",
            "No. Boxes: 5663\n",
            "No. Boxes: 5660\n",
            "No. Boxes: 5657\n",
            "No. Boxes: 5654\n",
            "No. Boxes: 5651\n",
            "No. Boxes: 5649\n",
            "No. Boxes: 5647\n",
            "No. Boxes: 5644\n",
            "No. Boxes: 5641\n",
            "No. Boxes: 5638\n",
            "No. Boxes: 5635\n",
            "No. Boxes: 5632\n",
            "No. Boxes: 5629\n",
            "No. Boxes: 5627\n",
            "No. Boxes: 5624\n",
            "No. Boxes: 5621\n",
            "No. Boxes: 5619\n",
            "No. Boxes: 5617\n",
            "No. Boxes: 5614\n",
            "No. Boxes: 5611\n",
            "No. Boxes: 5609\n",
            "No. Boxes: 5606\n",
            "No. Boxes: 5603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [01:32<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. Boxes: 5601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-729d98e7c594>\u001b[0m in \u001b[0;36m<cell line: 92>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-729d98e7c594>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0mcheck_class_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONF_THRESHOLD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fiinished Checking class accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m       pred_boxes, true_boxes = get_evaluation_bboxes(\n\u001b[0m\u001b[1;32m     73\u001b[0m           \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/YOLOV3_Pytorch/utils.py\u001b[0m in \u001b[0;36mget_evaluation_bboxes\u001b[0;34m(loader, model, iou_threshold, anchors, threshold, box_format, device)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processing batch index {idx}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Number of bounding boxes before NMS: {len(bboxes[idx])}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             nms_boxes = non_max_suppression(\n\u001b[0m\u001b[1;32m    325\u001b[0m                 \u001b[0mbboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                 \u001b[0miou_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miou_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/YOLOV3_Pytorch/utils.py\u001b[0m in \u001b[0;36mnon_max_suppression\u001b[0;34m(bboxes, iou_threshold, threshold, box_format)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mchosen_box\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         bboxes = [\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/YOLOV3_Pytorch/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mchosen_box\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             or intersection_over_union(\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchosen_box\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/YOLOV3_Pytorch/utils.py\u001b[0m in \u001b[0;36mintersection_over_union\u001b[0;34m(boxes_preds, boxes_labels, box_format)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mintersection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mbox1_area\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox1_x2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbox1_x1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbox1_y2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbox1_y1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mbox2_area\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox2_x2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbox2_x1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbox2_y2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbox2_y1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "importlib.reload(dataset)\n",
        "importlib.reload(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a59pNCrn82ih",
        "outputId": "43d8715e-08bd-46c2-ccbf-e9798435b0b2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'config' from '/content/drive/MyDrive/YOLOV3_Pytorch/config.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "RXBvtCWyLJCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file = \"/content/drive/MyDrive/datasets/YOLOV3_nuim/train.csv\"\n",
        "annotations = pd.read_csv(csv_file)\n",
        "dataset_dir = '/content/drive/MyDrive/datasets/YOLOV3_nuim'\n",
        "img_dir =  dataset_dir + '/images/'\n",
        "label_dir = dataset_dir + \"/labels/\"\n",
        "\n",
        "index = 14\n",
        "img_path = os.path.join(img_dir, annotations.iloc[index, 0])\n",
        "label_path = os.path.join(label_dir, annotations.iloc[index, 1])\n",
        "\n",
        "bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n",
        "\n",
        "img_path"
      ],
      "metadata": {
        "id": "xmq4O7_EURYY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "93355455-3af3-4a3b-a904-ef6bcfa98558"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/datasets/YOLOV3_nuim/images/00000031.jpg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [x y w h class]\n",
        "bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n",
        "img_path = os.path.join(img_dir, annotations.iloc[index, 0])\n",
        "image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "image.shape\n",
        "targets = [torch.zeros((3, S, S, 6)) for S in config.S]\n",
        "anchors = config.ANCHORS\n",
        "anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2])\n",
        "boxes = []\n",
        "box = bboxes[0]\n",
        "\n",
        "iou_anchors = iou_width_height(torch.tensor(box[2:4]), anchors)\n",
        "anchor_indices = iou_anchors.argsort(descending=True, dim=0)\n",
        "x, y, width, height, class_label = box\n",
        "has_anchor = [False,False, False]\n",
        "\n",
        "num_anchors = anchors.shape[0]\n",
        "num_anchors_per_scale = num_anchors // 3\n",
        "S = config.S\n",
        "\n",
        "x,y,width,height"
      ],
      "metadata": {
        "id": "mrDU5Qpedi0h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cae6c375-79fa-439a-94b2-842d12b64bc5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.73344, 0.57056, 0.15687, 0.20333)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "from albumentations.core import bbox_utils\n",
        "\n",
        "# Force reload of the module\n",
        "importlib.reload(bbox_utils)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN4r4-9qPwPK",
        "outputId": "993c7938-aa32-4c7c-830c-641c4fb9b87d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'albumentations.core.bbox_utils' from '/usr/local/lib/python3.10/dist-packages/albumentations/core/bbox_utils.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        " # Example bbox: [x_center, y_center, width, height]\n",
        "image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n",
        "\n",
        "# Transformation pipeline\n",
        "transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(height=416, width=416),\n",
        "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        "    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[]),\n",
        "\n",
        ")\n",
        "\n",
        "# Apply transformations\n",
        "transformed = transform(image=image, bboxes=bboxes)\n",
        "bboxes_0 = transformed[\"bboxes\"]\n",
        "# Convert list of tuples to NumPy array for efficient processing\n",
        "bboxes_clip = np.array(bboxes_0)\n",
        "# Clip x_center, y_center, width, height to [np.finfo(np.float32).tiny, 1.0]\n",
        "bboxes_clip[:, :4] = np.clip(bboxes_clip[:, :4], np.finfo(np.float16).tiny, 1.0)\n",
        "\n",
        "print(\"Original BBoxes:\", bboxes)\n",
        "print(\"Transformed BBoxes:\", transformed[\"bboxes\"])\n",
        "print(\"Clipped: \", bboxes_clip)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYzMxe5TwcrF",
        "outputId": "879a8e89-04f4-424f-813a-79f93145d34a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original BBoxes: [[0.73344, 0.57056, 0.15687, 0.20333, 4.0], [0.48219, 0.46056, 0.01188, 0.01667, 4.0], [0.24969, 0.49111, 0.08188, 0.04889, 4.0], [0.44188, 0.47778, 0.0275, 0.04444, 4.0], [0.555, 0.47667, 0.02, 0.03111, 4.0], [0.45469, 0.47, 0.01438, 0.02222, 4.0], [0.02937, 0.70389, 0.05875, 0.29, 4.0], [0.84562, 0.49833, 0.14125, 0.09222, 9.0], [0.91969, 0.61167, 0.16062, 0.37222, 4.0], [0.44844, 0.45667, 0.01562, 0.02, 4.0], [0.53406, 0.45667, 0.09813, 0.05556, 9.0], [0.52625, 0.47167, 0.02125, 0.03222, 4.0]]\n",
            "Transformed BBoxes: [[0.7334399819374084, 0.5705599784851074, 0.1568700075149536, 0.20333001017570496, 4.0], [0.48219001293182373, 0.46055999398231506, 0.011879980564117432, 0.01666998863220215, 4.0], [0.2496899962425232, 0.49111002683639526, 0.08187998831272125, 0.04889002442359924, 4.0], [0.4418799877166748, 0.4777800142765045, 0.0274999737739563, 0.04443997144699097, 4.0], [0.5550000071525574, 0.4766699969768524, 0.019999980926513672, 0.031109988689422607, 4.0], [0.45469000935554504, 0.4699999988079071, 0.01437997817993164, 0.02222001552581787, 4.0], [0.029372500255703926, 0.7038900256156921, 0.05874500051140785, 0.28999996185302734, 4.0], [0.8456199765205383, 0.4983299970626831, 0.14125001430511475, 0.09221997857093811, 9.0], [0.9196900129318237, 0.6116700172424316, 0.16061997413635254, 0.3722200393676758, 4.0], [0.44843998551368713, 0.45666998624801636, 0.015619993209838867, 0.019999980926513672, 4.0], [0.534060001373291, 0.45666998624801636, 0.0981299877166748, 0.05555999279022217, 9.0], [0.5262500047683716, 0.471670001745224, 0.021250009536743164, 0.03222000598907471, 4.0]]\n",
            "Clipped:  [[0.73343998 0.57055998 0.15687001 0.20333001 4.        ]\n",
            " [0.48219001 0.46055999 0.01187998 0.01666999 4.        ]\n",
            " [0.24969    0.49111003 0.08187999 0.04889002 4.        ]\n",
            " [0.44187999 0.47778001 0.02749997 0.04443997 4.        ]\n",
            " [0.55500001 0.47667    0.01999998 0.03110999 4.        ]\n",
            " [0.45469001 0.47       0.01437998 0.02222002 4.        ]\n",
            " [0.0293725  0.70389003 0.058745   0.28999996 4.        ]\n",
            " [0.84561998 0.49833    0.14125001 0.09221998 9.        ]\n",
            " [0.91969001 0.61167002 0.16061997 0.37222004 4.        ]\n",
            " [0.44843999 0.45666999 0.01561999 0.01999998 4.        ]\n",
            " [0.53406    0.45666999 0.09812999 0.05555999 9.        ]\n",
            " [0.52625    0.47167    0.02125001 0.03222001 4.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "num_anchors_per_scale = 3  # Adjust based on your model\n",
        "anchors = torch.tensor([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]])  # Example anchor dimensions\n",
        "\n",
        "\n",
        "def validate_box_coordinates(label_dir, annotations):\n",
        "    for index in range(len(annotations)):\n",
        "            # Load bounding boxes for the current index\n",
        "            label_path = os.path.join(label_dir, annotations.iloc[index, 1])\n",
        "            bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n",
        "            img_path = os.path.join(img_dir, annotations.iloc[index, 0])\n",
        "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "            for box in bboxes:\n",
        "                x, y, width, height, class_id = box[:5]\n",
        "                iou_anchors = iou_width_height(torch.tensor(box[2:4]), anchors)\n",
        "                anchor_indices = iou_anchors.argsort(descending=True, dim=0)\n",
        "                has_anchor = [False, False, False]\n",
        "\n",
        "                for anchor_idx in anchor_indices:\n",
        "                    # Calculate scale and anchor indices\n",
        "                    scale_idx = anchor_idx // num_anchors_per_scale\n",
        "                    anchor_on_scale = anchor_idx % num_anchors_per_scale\n",
        "                    S_0 = S[scale_idx]\n",
        "                    i, j = int(S_0 * y), int(S_0 * x)\n",
        "                    anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n",
        "                    if not anchor_taken and not has_anchor[scale_idx]:\n",
        "                        targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n",
        "                        x_cell, y_cell = S_0 * x - j, S_0 * y - i\n",
        "                        width_cell, height_cell = width * S_0, height * S_0\n",
        "\n",
        "                        # Create box coordinates tensor\n",
        "                        box_coordinates = torch.tensor([x_cell, y_cell, width_cell, height_cell])\n",
        "\n",
        "                        targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n",
        "                        targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n",
        "                        has_anchor[scale_idx] = True\n",
        "                        print(targets[scale_idx][anchor_on_scale, i, j, 1:5])\n",
        "                    elif not anchor_taken and iou_anchors[anchor_idx] > 0.5:\n",
        "                        targets[scale_idx][anchor_on_scale, i, j, 0] = -1  # ignore prediction\n",
        "\n",
        "\n",
        "    return image, tuple(targets)\n",
        "\n",
        "# Run the validation\n",
        "hello = validate_box_coordinates(label_dir, annotations)\n"
      ],
      "metadata": {
        "id": "f5RL8SQDrIQ3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}